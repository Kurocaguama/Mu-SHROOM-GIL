{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f01e38f-c02c-43a7-9a7e-f1b480e67e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "\n",
    "import argparse as ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9b446bc-2897-4758-815f-1f54450f8739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recompute_hard_labels(soft_labels):\n",
    "    \"\"\"optionally, infer hard labels from the soft labels provided\"\"\"\n",
    "    hard_labels = [] \n",
    "    prev_end = -1\n",
    "    for start, end in (\n",
    "        (lbl['start'], lbl['end']) \n",
    "        for lbl in sorted(soft_labels, key=lambda span: (span['start'], span['end']))\n",
    "        if lbl['prob'] > 0.5\n",
    "    ):\n",
    "        if start == prev_end:\n",
    "            hard_labels[-1][-1] = end\n",
    "        else:\n",
    "            hard_labels.append([start, end])\n",
    "        prev_end = end\n",
    "    return hard_labels\n",
    "\n",
    "\n",
    "def infer_soft_labels(hard_labels):\n",
    "    \"\"\"reformat hard labels into soft labels with prob 1\"\"\"\n",
    "    return [\n",
    "        {\n",
    "            'start': start,\n",
    "            'end': end,\n",
    "            'prob': 1.0,\n",
    "        }\n",
    "        for start, end in hard_labels\n",
    "    ]\n",
    "\n",
    "\n",
    "def load_jsonl_file_to_records(filename, is_ref=True):\n",
    "    \"\"\"read data from a JSONL file and format that as a `pandas.DataFrame`.\n",
    "    Performs minor format checks (ensures that some labels are present,\n",
    "    optionally compute missing labels on the fly).\"\"\"\n",
    "    df = pd.read_json(filename, lines=True)\n",
    "    if not is_ref:\n",
    "        assert ('hard_labels' in df.columns) or ('soft_labels' in df.columns), \\\n",
    "            f'File {filename} contains no predicted label!'\n",
    "        if 'hard_labels' not in df.columns:\n",
    "            df['hard_labels'] = df.soft_labels.apply(recompute_hard_labels)\n",
    "        elif 'soft_labels' not in df.columns:\n",
    "            df['soft_labels'] = df.hard_labels.apply(infer_soft_labels)\n",
    "    # adding an extra column for convenience\n",
    "    columns = ['id', 'soft_labels', 'hard_labels']\n",
    "    if is_ref:\n",
    "        df['text_len'] = df.model_output_text.apply(len)\n",
    "        columns += ['text_len']\n",
    "    df = df[columns]\n",
    "    return df.sort_values('id').to_dict(orient='records')\n",
    "\n",
    "def score_iou(ref_dict, pred_dict):\n",
    "    \"\"\"computes intersection-over-union between reference and predicted hard labels, for a single datapoint.\n",
    "    inputs:\n",
    "    - ref_dict: a gold reference datapoint,\n",
    "    - pred_dict: a model's prediction\n",
    "    returns:\n",
    "    the IoU, or 1.0 if neither the reference nor the prediction contain hallucinations\n",
    "    \"\"\"\n",
    "    # ensure the prediction is correctly matched to its reference\n",
    "    assert ref_dict['id'] == pred_dict['id']\n",
    "    # convert annotations to sets of indices\n",
    "    ref_indices = {idx for span in ref_dict['hard_labels'] for idx in range(*span)}\n",
    "    pred_indices = {idx for span in pred_dict['hard_labels'] for idx in range(*span)}\n",
    "    # avoid division by zero\n",
    "    if not pred_indices and not ref_indices: return 1.\n",
    "    # otherwise compute & return IoU\n",
    "    return len(ref_indices & pred_indices) / len(ref_indices | pred_indices)\n",
    "\n",
    "def score_cor(ref_dict, pred_dict):\n",
    "    \"\"\"computes Spearman correlation between predicted and reference soft labels, for a single datapoint.\n",
    "    inputs:\n",
    "    - ref_dict: a gold reference datapoint,\n",
    "    - pred_dict: a model's prediction\n",
    "    returns:\n",
    "    the Spearman correlation, or a binarized exact match (0.0 or 1.0) if the reference or prediction contains no variation\n",
    "    \"\"\"\n",
    "    # ensure the prediction is correctly matched to its reference\n",
    "    assert ref_dict['id'] == pred_dict['id']\n",
    "    # convert annotations to vectors of observations\n",
    "    ref_vec = [0.] * ref_dict['text_len']\n",
    "    pred_vec = [0.] * ref_dict['text_len']\n",
    "    for span in ref_dict['soft_labels']:\n",
    "        for idx in range(span['start'], span['end']):\n",
    "            ref_vec[idx] = span['prob']\n",
    "    for span in pred_dict['soft_labels']:\n",
    "        for idx in range(span['start'], span['end']):\n",
    "            pred_vec[idx] = span['prob']\n",
    "    # constant series (i.e., no hallucination) => cor is undef\n",
    "    if len({round(flt, 8) for flt in pred_vec}) == 1 or len({round(flt, 8) for flt in ref_vec}) == 1 : \n",
    "        return float(len({round(flt, 8) for flt in ref_vec}) == len({round(flt, 8) for flt in pred_vec}))\n",
    "    # otherwise compute Spearman's rho\n",
    "    return spearmanr(ref_vec, pred_vec).correlation\n",
    "\n",
    "def main(ref_dicts, pred_dicts, output_file=None):\n",
    "    assert len(ref_dicts) == len(pred_dicts)\n",
    "    ious = np.array([score_iou(r, d) for r, d in zip(ref_dicts, pred_dicts)])\n",
    "    cors = np.array([score_cor(r, d) for r, d in zip(ref_dicts, pred_dicts)])\n",
    "    if output_file is not None:\n",
    "        with open(output_file, 'w') as ostr:\n",
    "            print(f'IoU: {ious.mean():.8f}', file=ostr)\n",
    "            print(f'Cor: {cors.mean():.8f}', file=ostr)\n",
    "    return ious, cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c5afbd-8d4a-4c0c-b443-d1046fe585f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
