{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71af1f5d-2500-4178-8fca-3670ef26370c",
   "metadata": {},
   "source": [
    "# Esto es el código correspondiente a todo el flujo de trabajo.\n",
    "\n",
    "Está dividido en distintas celdas donde cada una contiene las funciones de cada parte del procesamiento.\n",
    "\n",
    "1. **Celda 1**: Obtención de contexto. Procesamiento de preguntas, extracción de páginas de Wikipedia, generación de resumen (_ground truth_).\n",
    "2. **Celda 2**: Llamada a OpenAI. A partir del contexto obtenido previamente se contestan las preguntas del ds con GPT-4o (_ground truth GPT_).\n",
    "3. **Celda 3**: Comparación entre las respuestas del dataset de Shroom con las respuestas que nosotros estamos generando, y generación de DS final.\n",
    "4. **Celda 4**: Evaluación (Script oficial de Mushroom)\n",
    "5. **Próximamente**: Modificación de formato y evaluación. (Va a depender de que la celda 3 se expanda entre comparación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd39eff0-0dbf-458f-bea8-2b5fc2b04ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b4bb81cfee4ebd87572f50f17a46be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wikipedia\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import transformers \n",
    "import os\n",
    "import argparse as ap\n",
    "\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from openai import OpenAI\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "model = BGEM3FlagModel('BAAI/bge-m3', use_fp16 = True)\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db552bad-1f3f-4c1b-a432-dcb41dc566da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CELDA 1: CONTEXTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5bf62f8-e596-4aca-8df9-5fc3d74837ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 1: OBTENCIÓN DE CONTEXTO\n",
    "\n",
    "def noun_list(a, lang):\n",
    "    \"\"\"\n",
    "    Filtra la pregunta y obtiene las PoST relevantes.\n",
    "    \n",
    "    a = list; Lista de preguntas del dataset\n",
    "    lang = 'es' or 'en'; Idioma a trabajar\n",
    "    \"\"\"\n",
    "    if lang == 'es':\n",
    "        post_spacy = spacy.load(\"es_core_news_sm\")\n",
    "    else:\n",
    "        post_spacy = spacy.load(\"en_core_web_sm\")\n",
    "    noun_list = []\n",
    "    nums = ['0','1','2','3','4','5','6','7','8','9']\n",
    "\n",
    "    for _ in a:\n",
    "        doc = post_spacy(_)\n",
    "        sub_noun = []\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\" or token.pos_ == \"NUM\":\n",
    "                sub_noun.append(token.text)\n",
    "            if token.pos_ == \"ADJ\" and token.text[0] in nums:\n",
    "                sub_noun.append(token.text)\n",
    "        noun_list.append(sub_noun)\n",
    "    return noun_list\n",
    "\n",
    "\n",
    "def keyword_por_preg(n_list):\n",
    "    \"\"\"\n",
    "    Junta lista de PoST previo a pasarlo por el API de Wikipedia.\n",
    "    \n",
    "    n_list = list; Obtenida de la función noun_list().\n",
    "    \"\"\"\n",
    "    keyword_list = []\n",
    "    for i in n_list:\n",
    "        keyword = ''\n",
    "        for j in i:\n",
    "            keyword = keyword + j + ' '\n",
    "        keyword_list.append(keyword)\n",
    "    return keyword_list\n",
    "\n",
    "\n",
    "def get_wikipage(text, lang, page_total):\n",
    "    \"\"\"\n",
    "    Regresa las n páginas de Wikipedia más relevantes al query\n",
    "\n",
    "    text = str; Texto proveniente de la función keyword_por_preg()\n",
    "    lang = 'es' or 'en'; Lenguaje necesario para wikipedia\n",
    "    page_total = int; Cantidad de páginas a regresar\n",
    "    \"\"\"\n",
    "    if lang == 'es':\n",
    "        wikipedia.set_lang('es')\n",
    "    if lang == 'en':\n",
    "        wikipedia.set_lang('en')\n",
    "    page_title = wikipedia.search(text, results = page_total)\n",
    "    return page_title\n",
    "\n",
    "\n",
    "def wikipipeline(dataset, lang, page_count):\n",
    "    \"\"\"\n",
    "    Genera los resúmenes que sirven como contexto de cada pregunta. \n",
    "\n",
    "    dataset = pd.DataFrame ; El nombre del dataset a procesar\n",
    "    dataset = list; ya sea el nombre del dataset en formato dataset[\"model_input\"] o list(set(dataset[\"model_input\"]))\n",
    "    lang = 'es' or 'en'; Idioma a trabajar, debe de coincidir con el del dataset para no generar algo incoherente\n",
    "    page_count = int; Cantidad de páginas de Wikipedia a extraer\n",
    "    \"\"\"\n",
    "    \n",
    "    noun_list_perrona = noun_list(dataset, lang)\n",
    "    key_list = keyword_por_preg(noun_list_perrona)\n",
    "\n",
    "    resumen_list = []\n",
    "    question_list = []\n",
    "    iterador = 0\n",
    "    for i in key_list:\n",
    "        pages = get_wikipage(i, lang, page_count)\n",
    "        resumen = ''\n",
    "        for x in pages:\n",
    "            try:\n",
    "                page = wikipedia.WikipediaPage(x)\n",
    "                page_sum = page.summary\n",
    "                resumen = resumen + '' + page_sum\n",
    "            except wikipedia.exceptions.DisambiguationError: # Se usa para evitar problemas al encontrar la página adecuada.\n",
    "                #print(i, page)\n",
    "                print(i, x)\n",
    "            #page_sum = page.summary\n",
    "            #resumen = resumen + '' + page_sum\n",
    "        resumen_list.append(resumen)\n",
    "        question_list.append(dataset[iterador])\n",
    "        iterador += 1\n",
    "    return resumen_list, question_list\n",
    "\n",
    "\n",
    "def generate_embeddings(sum_list, q, ruta):\n",
    "    \"\"\"\n",
    "    Genera dataframe de embeddings, y los guarda en un directorio de nuestra elección.\n",
    "\n",
    "    sum_list = list ; Lista de resúmenes obtenida previamente.\n",
    "    q = list ; Lista de preguntas obtenida previamente.\n",
    "    ruta = str ; Directorio para guardar. \n",
    "    \"\"\"\n",
    "    len_list = [len(_) for _ in sum_list]\n",
    "    max_length = max(len_list)\n",
    "\n",
    "    embs = model.encode(\n",
    "        sum_list,\n",
    "        batch_size = 12,\n",
    "        max_length = max_length,\n",
    "    )['dense_vecs']\n",
    "\n",
    "    embs_loco = [_ for _ in embs]\n",
    "    dic = {'Embedding':embs_loco, 'Texto':sum_list, 'Keywords Pregunta': q}\n",
    "    embs_df = pd.DataFrame(data=dic)\n",
    "    embs_df.to_csv(ruta)\n",
    "    print(f\"Embedding guardados en la ruta {ruta} . Saludos\")\n",
    "\n",
    "\n",
    "def full_context_pipeline(dataset, lang, num, ruta):\n",
    "    \"\"\"\n",
    "    Ejecuta todo el pipeline (todo junto alv compa).\n",
    "    \n",
    "    dataset = pd.DataFrame[_column_name_] ;  Columna del dataset a trabajar\n",
    "    lang = 'en' or 'es'; Lenguaje a trabajar.\n",
    "    num = int; Cantidad de páginas de wikipedia a recolectar.\n",
    "    ruta = str; Dirección de guardado de dataset. (DEBE DE TENER NOMBRE DEL ARCHIVO)\n",
    "    \"\"\"\n",
    "    conjunto = list(set(dataset))\n",
    "    sum_set, q_set = wikipipeline(conjunto, lang, num)\n",
    "    generate_embeddings(sum_set, q_set, ruta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03f7887-0506-428e-b8e3-e22556da5cac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CELDA 2: OPENAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f6da454-b930-4ddc-8599-44fa6fa1ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions(path):\n",
    "    \"\"\"\n",
    "    Returns dataset questions in a list.\n",
    "\n",
    "    path = str; File location.\n",
    "    \"\"\"\n",
    "    dataset = pd.read_json(path, lines = True)\n",
    "    questions  = dataset[\"model_input\"]\n",
    "    return [_ for _ in questions]\n",
    "\n",
    "def get_embs(path):\n",
    "    \"\"\"\n",
    "    Returns embeddings as a pandas DataFrame.\n",
    "\n",
    "    path = str; File location.\n",
    "    \"\"\"\n",
    "    embs = pd.read_csv(path)\n",
    "    embs = embs.drop(columns = [\"Unnamed: 0\"])\n",
    "    embs_text = embs[\"Texto\"]\n",
    "    embs_vec = embs[\"Embedding\"]\n",
    "    embs_txt = [_ for _ in embs_text]\n",
    "    return embs_txt, embs_vec\n",
    "\n",
    "def gen_answer(ques, retr):\n",
    "    \"\"\"\n",
    "    Generates GPT based answer given a question and a context window.\n",
    "\n",
    "    ques = str; Question extracted from the task dataset.\n",
    "    retr = str; Context extracted from embeddings.\n",
    "    \"\"\"\n",
    "    client = OpenAI()\n",
    "    prompt = f\"\"\"\n",
    "        You are a bot that answers trivia questions.\n",
    "        Be brief, answer in short sentences highlighting important information.\n",
    "\n",
    "        This is the trivia question you need to answer:\n",
    "        {ques}.\n",
    "\n",
    "        This is text that you should use to answer the question:\n",
    "        {retr}.\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model = \"gpt-4o-mini\",\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            },\n",
    "        ],\n",
    "        temperature = 0.1,\n",
    "    ).choices[0].message\n",
    "    \n",
    "    return response.content\n",
    "\n",
    "\n",
    "def gpt_full(emb_dataset, ruta):\n",
    "    \"\"\"\n",
    "    Implementación completa del pipeline\n",
    "\n",
    "    emb_datset = pd.DataFrame ; Debe de ser el dataset que contenga el contexto de cada pregunta.\n",
    "    ruta = str ; Ruta para guardar el archivo.\n",
    "    \"\"\"\n",
    "    preguntas = emb_dataset[\"Keywords Pregunta\"]\n",
    "    contexto = emb_dataset[\"Texto\"]\n",
    "    gpt_answer = []\n",
    "    for i in range(len(contexto)):\n",
    "        answer = gen_answer(preguntas[i], contexto[i])\n",
    "        gpt_answer.append(answer)\n",
    "    dic = {'Pregunta': preguntas, 'Respuesta GPT': gpt_answer, 'Contexto': contexto}\n",
    "    full_ds = pd.DataFrame(data=dic)\n",
    "    full_ds.to_csv(ruta)\n",
    "    print(f\"Se guardó el dataset de respuestas de Chapi en: {ruta}. Viva Leo Messi gigante dios\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ac6597-d92a-4512-90f1-f7b3258e6b29",
   "metadata": {},
   "source": [
    "### CELDA 3: COMPARACIÓN Y AJUSTE DE FORMATO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d0100c1-cf2b-497a-b130-64c325b7f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_interval(texto):\n",
    "    \"\"\"\n",
    "    Regresa una partición por intervalos aleatoria.\n",
    "\n",
    "    texto = str; El texto a particionar\n",
    "    \"\"\"\n",
    "\n",
    "    char_pos = 0\n",
    "    intervalos = []\n",
    "    while(char_pos < len(texto)):\n",
    "        step = random.randint(1, len(texto) - char_pos)\n",
    "        char_aux = char_pos + step\n",
    "        intervalo_aux = [char_pos, char_aux]\n",
    "        intervalos.append(intervalo_aux)\n",
    "        char_pos += char_aux\n",
    "    if intervalos[-1][1] == len(texto):\n",
    "        return intervalos\n",
    "    else:\n",
    "        final_int = [char_aux, len(texto)]\n",
    "        intervalos.append(final_int)\n",
    "    return intervalos\n",
    "\n",
    "\n",
    "def random_probalist(probs_dic):\n",
    "    \"\"\"\n",
    "    ESTA ES UNA FUNCIÓN AUXILIAR\n",
    "    Genera los soft_labels de manera aleatoria a partir de una lista de segmentación aleatoria del texto.\n",
    "    \n",
    "    probs_dic = dict ; Diccionario de probabilidades auxiliar \n",
    "    \"\"\"\n",
    "    prob_list = []\n",
    "    for _ in interval_list:\n",
    "        aux_list = []\n",
    "        for i in _:\n",
    "            prob = random.randint(0,len(probs_dic))\n",
    "            proba_num = probs_dic[prob]\n",
    "            probabilidades = {'start':i[0], 'prob':proba_num, 'end':i[-1]}\n",
    "            aux_list.append(probabilidades)\n",
    "        prob_list.append(aux_list)\n",
    "    return prob_list\n",
    "\n",
    "\n",
    "def json_creation(base_ds, lista_probabilidad, ruta):\n",
    "    \"\"\"\n",
    "    Crea y guarda un dataset con el formato necesario para la evaluación.\n",
    "\n",
    "    base_ds = pd.DataFrame ;  Correspondiente al conjunto de train/test/val cuyos valores se van a modificar\n",
    "    lista_probabilidad = list ; Lista donde cada entrada corresponde a un diccionario con los \"soft_labels\" predecidos por nosotros.\n",
    "    ruta = str; Ubicación donde se guardará el dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    new_ds = base_ds.drop(columns = [\"soft_labels\", \"hard_labels\"])\n",
    "    new_ds[\"soft_labels\"] = lista_probabilidad\n",
    "    hard_labels = [recompute_hard_labels(new_ds[\"soft_labels\"][i] for i in range(len(new_ds[\"soft_labels\"])))]\n",
    "    new_ds[\"hard_labels\"] = hard_labels\n",
    "    new_ds = new_ds.iloc[;, [0,1,2,3,4,5,7,8,5,6]]\n",
    "    new_ds.to_json(ruta, orient = \"records\", lines = True)\n",
    "    print(f\"Se guardó el archivo en {ruta}. Viva Messi.\")\n",
    "\n",
    "\n",
    "def pipeline3(dataset, ruta):\n",
    "    \"\"\"\n",
    "    Tercer pipeline de corrido.\n",
    "\n",
    "    dataset = pd.DataFrame[\"model_output_text\"] ;  Dataset a comparar. DEBE DE SER EL ARCHIVO ORIGINAL\n",
    "    ruta = str ; Ruta para guardar\n",
    "    \"\"\"\n",
    "    dic_aux = {0:0, 1:0.333333, 2:0.6666666, 3:1}\n",
    "    random_split = [random_interval(_) for _ in dataset]\n",
    "    proba_list = random_probalist(random_split)\n",
    "    json_creation(dataset, proba_list, ruta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3cfb82-7938-48f6-8d61-d954fef6ffd8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CELDA 4: EVALUACIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6e4d2b4-9fa0-4429-9504-39849ff819f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recompute_hard_labels(soft_labels):\n",
    "    \"\"\"optionally, infer hard labels from the soft labels provided\"\"\"\n",
    "    hard_labels = [] \n",
    "    prev_end = -1\n",
    "    for start, end in (\n",
    "        (lbl['start'], lbl['end']) \n",
    "        for lbl in sorted(soft_labels, key=lambda span: (span['start'], span['end']))\n",
    "        if lbl['prob'] > 0.5\n",
    "    ):\n",
    "        if start == prev_end:\n",
    "            hard_labels[-1][-1] = end\n",
    "        else:\n",
    "            hard_labels.append([start, end])\n",
    "        prev_end = end\n",
    "    return hard_labels\n",
    "\n",
    "\n",
    "def infer_soft_labels(hard_labels):\n",
    "    \"\"\"reformat hard labels into soft labels with prob 1\"\"\"\n",
    "    return [\n",
    "        {\n",
    "            'start': start,\n",
    "            'end': end,\n",
    "            'prob': 1.0,\n",
    "        }\n",
    "        for start, end in hard_labels\n",
    "    ]\n",
    "\n",
    "\n",
    "def load_jsonl_file_to_records(filename, is_ref=True):\n",
    "    \"\"\"read data from a JSONL file and format that as a `pandas.DataFrame`.\n",
    "    Performs minor format checks (ensures that some labels are present,\n",
    "    optionally compute missing labels on the fly).\"\"\"\n",
    "    df = pd.read_json(filename, lines=True)\n",
    "    if not is_ref:\n",
    "        assert ('hard_labels' in df.columns) or ('soft_labels' in df.columns), \\\n",
    "            f'File {filename} contains no predicted label!'\n",
    "        if 'hard_labels' not in df.columns:\n",
    "            df['hard_labels'] = df.soft_labels.apply(recompute_hard_labels)\n",
    "        elif 'soft_labels' not in df.columns:\n",
    "            df['soft_labels'] = df.hard_labels.apply(infer_soft_labels)\n",
    "    # adding an extra column for convenience\n",
    "    columns = ['id', 'soft_labels', 'hard_labels']\n",
    "    if is_ref:\n",
    "        df['text_len'] = df.model_output_text.apply(len)\n",
    "        columns += ['text_len']\n",
    "    df = df[columns]\n",
    "    return df.sort_values('id').to_dict(orient='records')\n",
    "\n",
    "def score_iou(ref_dict, pred_dict):\n",
    "    \"\"\"computes intersection-over-union between reference and predicted hard labels, for a single datapoint.\n",
    "    inputs:\n",
    "    - ref_dict: a gold reference datapoint,\n",
    "    - pred_dict: a model's prediction\n",
    "    returns:\n",
    "    the IoU, or 1.0 if neither the reference nor the prediction contain hallucinations\n",
    "    \"\"\"\n",
    "    # ensure the prediction is correctly matched to its reference\n",
    "    assert ref_dict['id'] == pred_dict['id']\n",
    "    # convert annotations to sets of indices\n",
    "    ref_indices = {idx for span in ref_dict['hard_labels'] for idx in range(*span)}\n",
    "    pred_indices = {idx for span in pred_dict['hard_labels'] for idx in range(*span)}\n",
    "    # avoid division by zero\n",
    "    if not pred_indices and not ref_indices: return 1.\n",
    "    # otherwise compute & return IoU\n",
    "    return len(ref_indices & pred_indices) / len(ref_indices | pred_indices)\n",
    "\n",
    "def score_cor(ref_dict, pred_dict):\n",
    "    \"\"\"computes Spearman correlation between predicted and reference soft labels, for a single datapoint.\n",
    "    inputs:\n",
    "    - ref_dict: a gold reference datapoint,\n",
    "    - pred_dict: a model's prediction\n",
    "    returns:\n",
    "    the Spearman correlation, or a binarized exact match (0.0 or 1.0) if the reference or prediction contains no variation\n",
    "    \"\"\"\n",
    "    # ensure the prediction is correctly matched to its reference\n",
    "    assert ref_dict['id'] == pred_dict['id']\n",
    "    # convert annotations to vectors of observations\n",
    "    ref_vec = [0.] * ref_dict['text_len']\n",
    "    pred_vec = [0.] * ref_dict['text_len']\n",
    "    for span in ref_dict['soft_labels']:\n",
    "        for idx in range(span['start'], span['end']):\n",
    "            ref_vec[idx] = span['prob']\n",
    "    for span in pred_dict['soft_labels']:\n",
    "        for idx in range(span['start'], span['end']):\n",
    "            pred_vec[idx] = span['prob']\n",
    "    # constant series (i.e., no hallucination) => cor is undef\n",
    "    if len({round(flt, 8) for flt in pred_vec}) == 1 or len({round(flt, 8) for flt in ref_vec}) == 1 : \n",
    "        return float(len({round(flt, 8) for flt in ref_vec}) == len({round(flt, 8) for flt in pred_vec}))\n",
    "    # otherwise compute Spearman's rho\n",
    "    return spearmanr(ref_vec, pred_vec).correlation\n",
    "\n",
    "def main(ref_dicts, pred_dicts, output_file=None):\n",
    "    assert len(ref_dicts) == len(pred_dicts)\n",
    "    ious = np.array([score_iou(r, d) for r, d in zip(ref_dicts, pred_dicts)])\n",
    "    cors = np.array([score_cor(r, d) for r, d in zip(ref_dicts, pred_dicts)])\n",
    "    if output_file is not None:\n",
    "        with open(output_file, 'w') as ostr:\n",
    "            print(f'IoU: {ious.mean():.8f}', file=ostr)\n",
    "            print(f'Cor: {cors.mean():.8f}', file=ostr)\n",
    "    return ious, cors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48529795-924b-41fe-b1b3-e24276fb9269",
   "metadata": {},
   "source": [
    "## CASO DE USO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87f41b1c-8575-4206-aa26-11efcd110b81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traits  Trait\n",
      "techniques  Technique\n",
      "Stahlberg  Stahlberg (disambiguation)\n",
      "region France Vaux en Amiénois  Vaux\n",
      "Pasteur crater  Pasteur (disambiguation)\n",
      "Black Sabbath Eternal Idol  Eternal Idol\n",
      "19521 Chaos  Chaos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding guardados en la ruta C:\\Users\\FLopezP\\Documents\\GitHub\\Mu-SHROOM-GIL\\Datasets\\full_pipeline_datasets\\en_context.csv . Saludos\n",
      "CPU times: total: 13.3 s\n",
      "Wall time: 4min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Celda 1\n",
    "test_set_en = pd.read_json(r'C:\\Users\\FLopezP\\Documents\\GitHub\\Mu-SHROOM-GIL\\Datasets\\test_ds\\v1\\mushroom.en-tst.v1.jsonl', lines = True)\n",
    "full_context_pipeline(test_set_en[\"model_input\"], 'en', 2, r'C:\\Users\\FLopezP\\Documents\\GitHub\\Mu-SHROOM-GIL\\Datasets\\full_pipeline_datasets\\en_context.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "660c569f-ac6d-490f-8d35-272728135167",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-AGQH6BhmQW0c8kz43DO15Fc3 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:4\u001b[0m\n",
      "Cell \u001b[1;32mIn[12], line 69\u001b[0m, in \u001b[0;36mgpt_full\u001b[1;34m(emb_dataset, ruta)\u001b[0m\n\u001b[0;32m     67\u001b[0m gpt_answer \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(contexto)):\n\u001b[1;32m---> 69\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgen_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreguntas\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontexto\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     gpt_answer\u001b[38;5;241m.\u001b[39mappend(answer)\n\u001b[0;32m     71\u001b[0m dic \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPregunta\u001b[39m\u001b[38;5;124m'\u001b[39m: preguntas, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRespuesta GPT\u001b[39m\u001b[38;5;124m'\u001b[39m: gpt_answer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContexto\u001b[39m\u001b[38;5;124m'\u001b[39m: contexto}\n",
      "Cell \u001b[1;32mIn[12], line 43\u001b[0m, in \u001b[0;36mgen_answer\u001b[1;34m(ques, retr)\u001b[0m\n\u001b[0;32m     31\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI()\n\u001b[0;32m     32\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124m    You are a bot that answers trivia questions.\u001b[39m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124m    Be brief, answer in short sentences highlighting important information.\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m---> 43\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-4o-mini\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a helpful assistant\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\openai\\_utils\\_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\openai\\resources\\chat\\completions.py:742\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    739\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    740\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    741\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 742\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    760\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    761\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    762\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    763\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    764\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    765\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    766\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    767\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    768\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    769\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    772\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    773\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    774\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    777\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\openai\\_base_client.py:1270\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1257\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1258\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1265\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1267\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1268\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1269\u001b[0m     )\n\u001b[1;32m-> 1270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\openai\\_base_client.py:947\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    945\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 947\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\openai\\_base_client.py:1036\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1035\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1036\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1043\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\openai\\_base_client.py:1085\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1085\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1091\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\openai\\_base_client.py:1036\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1035\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1036\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1043\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\openai\\_base_client.py:1085\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1085\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1091\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\openai\\_base_client.py:1051\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1048\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1050\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1051\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1054\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1055\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1059\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1060\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-AGQH6BhmQW0c8kz43DO15Fc3 on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Celda 2\n",
    "\n",
    "en_embs = pd.read_csv(r'C:\\Users\\FLopezP\\Documents\\GitHub\\Mu-SHROOM-GIL\\Datasets\\full_pipeline_datasets\\en_context.csv')\n",
    "gpt_full(en_embs, r'C:\\Users\\FLopezP\\Documents\\GitHub\\Mu-SHROOM-GIL\\Datasets\\full_pipeline_datasets\\en_gpt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8082a5e-a14c-4182-8687-c504e0038bd0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed eval>:2\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pipeline3' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Celda 3\n",
    "pipeline3(test_set_en, r'C:\\Users\\FLopezP\\Documents\\GitHub\\Mu-SHROOM-GIL\\Datasets\\full_pipeline_datasets\\en_test_dataset.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e27440-3abf-4c79-9e5f-10093d06dcdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
