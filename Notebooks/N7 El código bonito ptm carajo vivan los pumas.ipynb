{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71af1f5d-2500-4178-8fca-3670ef26370c",
   "metadata": {},
   "source": [
    "# Esto es el código correspondiente a todo el flujo de trabajo.\n",
    "\n",
    "Está dividido en distintas celdas donde cada una contiene las funciones de cada parte del procesamiento.\n",
    "\n",
    "1. **Celda 1**: Obtención de contexto. Procesamiento de preguntas, extracción de páginas de Wikipedia, generación de resumen (_ground truth_).\n",
    "2. **Celda 2**: Llamada a OpenAI. A partir del contexto obtenido previamente se contestan las preguntas del ds con GPT-4o (_ground truth GPT_).\n",
    "3. **Celda 3**: Comparación entre las respuestas del dataset de Shroom con las respuestas que nosotros estamos generando, y generación de DS final.\n",
    "4. **Celda 4**: Evaluación (Script oficial de Mushroom)\n",
    "5. **Próximamente**: Modificación de formato y evaluación. (Va a depender de que la celda 3 se expanda entre comparación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd39eff0-0dbf-458f-bea8-2b5fc2b04ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34460c980ae64c9fb925ad5ff8bf7001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eef59d989b8848fdab08c58ee8618410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wikipedia\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import transformers \n",
    "import os\n",
    "import argparse as ap\n",
    "import time\n",
    "\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "from openai import OpenAI\n",
    "from scipy.stats import spearmanr\n",
    "from transformers import pipeline\n",
    "from huggingface_hub import login\n",
    "\n",
    "model = BGEM3FlagModel('BAAI/bge-m3', use_fp16 = True)\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "login(token = \"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\")\n",
    "\n",
    "model_instruct = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "pipe1 = pipeline(\n",
    "    \"text-generation\",\n",
    "    model = model_instruct,\n",
    "    torch_dtype = torch.bfloat16,\n",
    "    device_map = \"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db552bad-1f3f-4c1b-a432-dcb41dc566da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CELDA 1: CONTEXTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5bf62f8-e596-4aca-8df9-5fc3d74837ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELDA 1: OBTENCIÓN DE CONTEXTO\n",
    "\n",
    "def noun_list(a, lang):\n",
    "    \"\"\"\n",
    "    Filtra la pregunta y obtiene las PoST relevantes.\n",
    "    \n",
    "    a = list; Lista de preguntas del dataset\n",
    "    lang = 'es' or 'en'; Idioma a trabajar\n",
    "    \"\"\"\n",
    "    if lang == 'es':\n",
    "        post_spacy = spacy.load(\"es_core_news_sm\")\n",
    "    else:\n",
    "        post_spacy = spacy.load(\"en_core_web_sm\")\n",
    "    noun_list = []\n",
    "    nums = ['0','1','2','3','4','5','6','7','8','9']\n",
    "\n",
    "    for _ in a:\n",
    "        doc = post_spacy(_)\n",
    "        sub_noun = []\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\" or token.pos_ == \"NUM\":\n",
    "                sub_noun.append(token.text)\n",
    "            if token.pos_ == \"ADJ\" and token.text[0] in nums:\n",
    "                sub_noun.append(token.text)\n",
    "        noun_list.append(sub_noun)\n",
    "    return noun_list\n",
    "\n",
    "\n",
    "def keyword_por_preg(n_list):\n",
    "    \"\"\"\n",
    "    Junta lista de PoST previo a pasarlo por el API de Wikipedia.\n",
    "    \n",
    "    n_list = list; Obtenida de la función noun_list().\n",
    "    \"\"\"\n",
    "    keyword_list = []\n",
    "    for i in n_list:\n",
    "        keyword = ''\n",
    "        for j in i:\n",
    "            keyword = keyword + j + ' '\n",
    "        keyword_list.append(keyword)\n",
    "    return keyword_list\n",
    "\n",
    "\n",
    "def get_wikipage(text, lang, page_total):\n",
    "    \"\"\"\n",
    "    Regresa las n páginas de Wikipedia más relevantes al query\n",
    "\n",
    "    text = str; Texto proveniente de la función keyword_por_preg()\n",
    "    lang = 'es' or 'en'; Lenguaje necesario para wikipedia\n",
    "    page_total = int; Cantidad de páginas a regresar\n",
    "    \"\"\"\n",
    "    if lang == 'es':\n",
    "        wikipedia.set_lang('es')\n",
    "    if lang == 'en':\n",
    "        wikipedia.set_lang('en')\n",
    "    page_title = wikipedia.search(text, results = page_total)\n",
    "    return page_title\n",
    "\n",
    "\n",
    "def wikipipeline(dataset, lang, page_count):\n",
    "    \"\"\"\n",
    "    Genera los resúmenes que sirven como contexto de cada pregunta. \n",
    "\n",
    "    dataset = pd.DataFrame ; El nombre del dataset a procesar\n",
    "    dataset = list; ya sea el nombre del dataset en formato dataset[\"model_input\"] o list(set(dataset[\"model_input\"]))\n",
    "    lang = 'es' or 'en'; Idioma a trabajar, debe de coincidir con el del dataset para no generar algo incoherente\n",
    "    page_count = int; Cantidad de páginas de Wikipedia a extraer\n",
    "    \"\"\"\n",
    "    \n",
    "    noun_list_perrona = noun_list(dataset, lang)\n",
    "    key_list = keyword_por_preg(noun_list_perrona)\n",
    "\n",
    "    resumen_list = []\n",
    "    question_list = []\n",
    "    iterador = 0\n",
    "    for i in key_list:\n",
    "        pages = get_wikipage(i, lang, page_count)\n",
    "        resumen = ''\n",
    "        for x in pages:\n",
    "            try:\n",
    "                page = wikipedia.WikipediaPage(x)\n",
    "                page_sum = page.summary\n",
    "                resumen = resumen + '' + page_sum\n",
    "            except wikipedia.exceptions.DisambiguationError: # Se usa para evitar problemas al encontrar la página adecuada.\n",
    "                #print(i, page)\n",
    "                print(i, x)\n",
    "            #page_sum = page.summary\n",
    "            #resumen = resumen + '' + page_sum\n",
    "        resumen_list.append(resumen)\n",
    "        question_list.append(dataset[iterador])\n",
    "        iterador += 1\n",
    "    return resumen_list, question_list\n",
    "\n",
    "\n",
    "def generate_embeddings(sum_list, q, ruta):\n",
    "    \"\"\"\n",
    "    Genera dataframe de embeddings, y los guarda en un directorio de nuestra elección.\n",
    "\n",
    "    sum_list = list ; Lista de resúmenes obtenida previamente.\n",
    "    q = list ; Lista de preguntas obtenida previamente.\n",
    "    ruta = str ; Directorio para guardar. \n",
    "    \"\"\"\n",
    "    len_list = [len(_) for _ in sum_list]\n",
    "    max_length = max(len_list)\n",
    "\n",
    "    embs = model.encode(\n",
    "        sum_list,\n",
    "        batch_size = 12,\n",
    "        max_length = max_length,\n",
    "    )['dense_vecs']\n",
    "\n",
    "    embs_loco = [_ for _ in embs]\n",
    "    dic = {'Embedding':embs_loco, 'Texto':sum_list, 'Keywords Pregunta': q}\n",
    "    embs_df = pd.DataFrame(data=dic)\n",
    "    embs_df.to_csv(ruta)\n",
    "    print(f\"Embedding guardados en la ruta {ruta} . Saludos\")\n",
    "\n",
    "\n",
    "def full_context_pipeline(dataset, lang, num, ruta):\n",
    "    \"\"\"\n",
    "    Ejecuta todo el pipeline (todo junto alv compa).\n",
    "    \n",
    "    dataset = pd.DataFrame[_column_name_] ;  Columna del dataset a trabajar\n",
    "    lang = 'en' or 'es'; Lenguaje a trabajar.\n",
    "    num = int; Cantidad de páginas de wikipedia a recolectar.\n",
    "    ruta = str; Dirección de guardado de dataset. (DEBE DE TENER NOMBRE DEL ARCHIVO)\n",
    "    \"\"\"\n",
    "    conjunto = list(set(dataset))\n",
    "    sum_set, q_set = wikipipeline(conjunto, lang, num)\n",
    "    generate_embeddings(sum_set, q_set, ruta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03f7887-0506-428e-b8e3-e22556da5cac",
   "metadata": {},
   "source": [
    "### CELDA 2: OPENAI y LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f6da454-b930-4ddc-8599-44fa6fa1ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_questions(path):\n",
    "    \"\"\"\n",
    "    Returns dataset questions in a list.\n",
    "\n",
    "    path = str; File location.\n",
    "    \"\"\"\n",
    "    dataset = pd.read_json(path, lines = True)\n",
    "    questions  = dataset[\"model_input\"]\n",
    "    return [_ for _ in questions]\n",
    "\n",
    "def get_embs(path):\n",
    "    \"\"\"\n",
    "    Returns embeddings as a pandas DataFrame.\n",
    "\n",
    "    path = str; File location.\n",
    "    \"\"\"\n",
    "    embs = pd.read_csv(path)\n",
    "    embs = embs.drop(columns = [\"Unnamed: 0\"])\n",
    "    embs_text = embs[\"Texto\"]\n",
    "    embs_vec = embs[\"Embedding\"]\n",
    "    embs_txt = [_ for _ in embs_text]\n",
    "    return embs_txt, embs_vec\n",
    "\n",
    "\n",
    "def llama_gen(ques, context):\n",
    "    prompt = f\"\"\"\n",
    "        You are a bot that answers trivia questions.\n",
    "        Be brief, answer in short sentences highlighting important information.\n",
    "        If the given text doesn't answer the question, answer as truthfully as you can with your own information.\n",
    "        \n",
    "        This is the trivia question you need to answer:\n",
    "        {ques}\n",
    "        \n",
    "        This is the text that you should use:\n",
    "        {context}\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a chatbot that responds to general knowledge questions with high fidelity based on a given text. If the text doesn't give the correct information, answer with your own information.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    outputs = pipe1(\n",
    "        messages,\n",
    "        max_new_tokens = 256,\n",
    "    )\n",
    "    return outputs[0][\"generated_text\"][-1]\n",
    "    \n",
    "\n",
    "def gen_answer(ques, retr):\n",
    "    \"\"\"\n",
    "    Generates GPT based answer given a question and a context window.\n",
    "\n",
    "    ques = str; Question extracted from the task dataset.\n",
    "    retr = str; Context extracted from embeddings.\n",
    "    \"\"\"\n",
    "    client = OpenAI()\n",
    "    prompt = f\"\"\"\n",
    "        You are a bot that answers trivia questions.\n",
    "        Be brief, answer in short sentences highlighting important information.\n",
    "        If the following text doesn't answer the question, answer as truthfully as you can.\n",
    "\n",
    "        This is the trivia question you need to answer:\n",
    "        {ques}.\n",
    "\n",
    "        This is text that you should use to answer the question:\n",
    "        {retr}.\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model = \"gpt-4o-mini\",\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            },\n",
    "        ],\n",
    "        temperature = 0.1,\n",
    "    ).choices[0].message\n",
    "    \n",
    "    return response.content\n",
    "\n",
    "\n",
    "def llm_full(emb_dataset, ruta, llm):\n",
    "    \"\"\"\n",
    "    Implementación completa del pipeline\n",
    "\n",
    "    emb_datset = pd.DataFrame ; Debe de ser el dataset que contenga el contexto de cada pregunta.\n",
    "    ruta = str ; Ruta para guardar el archivo.\n",
    "    llm = str ; gpt o llama -> Para elegir el LLM a usar.\n",
    "    \"\"\"\n",
    "    preguntas = emb_dataset[\"Keywords Pregunta\"]\n",
    "    contexto = emb_dataset[\"Texto\"]\n",
    "    llm_answer = []\n",
    "    for i in range(len(contexto)):\n",
    "        if llm == 'gpt':\n",
    "            answer = gen_answer(preguntas[i], contexto[i])\n",
    "        elif llm == 'llama':\n",
    "            answer = llama_gen(preguntas[i], contexto[i])\n",
    "        llm_answer.append(answer)\n",
    "    dic = {'Pregunta': preguntas, 'Respuesta LLM': llm_answer, 'Contexto': contexto}\n",
    "    full_ds = pd.DataFrame(data=dic)\n",
    "    full_ds.to_csv(ruta)\n",
    "    print(f\"Se guardó el dataset de respuestas de Chapi en: {ruta}. Viva Leo Messi gigante dios\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ac6597-d92a-4512-90f1-f7b3258e6b29",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CELDA 3: COMPARACIÓN Y AJUSTE DE FORMATO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49be8e30-9b7f-45f2-8380-debfd6b8a65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0 Beta\n",
    "# El código de comparación de Karla Denia\n",
    "# HAY QUE INCORPORARLO AL PIPELINE TOTAL.\n",
    "\n",
    "def get_hard_labels(claims, evidences):\n",
    "  hard_labels = []\n",
    "  for c, e in zip(claims, evidences):\n",
    "    diff = list(set(c.split()) - set(e.split()))\n",
    "    label = []\n",
    "    for i in diff:\n",
    "      start = c.index(i)\n",
    "      label.append([start, start+len(i)])\n",
    "    hard_labels.append(label)\n",
    "  return hard_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3d0100c1-cf2b-497a-b130-64c325b7f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_interval(texto):\n",
    "    \"\"\"\n",
    "    Regresa una partición por intervalos aleatoria.\n",
    "\n",
    "    texto = str; El texto a particionar\n",
    "    \"\"\"\n",
    "\n",
    "    char_pos = 0\n",
    "    intervalos = []\n",
    "    while(char_pos < len(texto)):\n",
    "        step = random.randint(1, len(texto) - char_pos)\n",
    "        char_aux = char_pos + step\n",
    "        intervalo_aux = [char_pos, char_aux]\n",
    "        intervalos.append(intervalo_aux)\n",
    "        char_pos += char_aux\n",
    "    if intervalos[-1][1] == len(texto):\n",
    "        return intervalos\n",
    "    else:\n",
    "        final_int = [char_aux, len(texto)]\n",
    "        intervalos.append(final_int)\n",
    "    return intervalos\n",
    "\n",
    "\n",
    "def random_probalist(probs_dic, interval_list):\n",
    "    \"\"\"\n",
    "    ESTA ES UNA FUNCIÓN AUXILIAR\n",
    "    Genera los soft_labels de manera aleatoria a partir de una lista de segmentación aleatoria del texto.\n",
    "    \n",
    "    probs_dic = dict ; Diccionario de probabilidades auxiliar \n",
    "    interval_list = list ; Obtenida de random_interval()\n",
    "    \"\"\"\n",
    "    prob_list = []\n",
    "    for _ in interval_list:\n",
    "        aux_list = []\n",
    "        for i in _:\n",
    "            prob = random.randint(0,len(probs_dic)-1)\n",
    "            proba_num = probs_dic[prob]\n",
    "            probabilidades = {'start':i[0], 'prob':proba_num, 'end':i[-1]}\n",
    "            aux_list.append(probabilidades)\n",
    "        prob_list.append(aux_list)\n",
    "    return prob_list\n",
    "\n",
    "\n",
    "def json_creation(base_ds, lista_probabilidad, ruta):\n",
    "    \"\"\"\n",
    "    Crea y guarda un dataset con el formato necesario para la evaluación.\n",
    "\n",
    "    base_ds = pd.DataFrame ;  Correspondiente al conjunto de train/test/val cuyos valores se van a modificar\n",
    "    lista_probabilidad = list ; Lista donde cada entrada corresponde a un diccionario con los \"soft_labels\" predecidos por nosotros.\n",
    "    ruta = str; Ubicación donde se guardará el dataset.\n",
    "    \"\"\"\n",
    "    if \"soft_labels\" and 'hard_labels' in base_ds.columns:\n",
    "        new_ds = base_ds.drop(columns = [\"soft_labels\", \"hard_labels\"])\n",
    "    else:\n",
    "        new_ds = base_ds.copy()\n",
    "    new_ds[\"soft_labels\"] = lista_probabilidad\n",
    "    hard_labels = [recompute_hard_labels(new_ds[\"soft_labels\"][i]) for i in range(len(new_ds[\"soft_labels\"]))]\n",
    "    new_ds[\"hard_labels\"] = hard_labels\n",
    "    new_ds = new_ds.iloc[:, [0,1,2,3,4,5,7,8,6]]\n",
    "    new_ds = new_ds.reset_index()\n",
    "    new_ds.to_json(ruta, orient = \"records\", lines = True)\n",
    "    print(f\"Se guardó el archivo en {ruta}. Viva Messi.\")\n",
    "\n",
    "\n",
    "def pipeline3(dataset, ruta):\n",
    "    \"\"\"\n",
    "    Tercer pipeline de corrido.\n",
    "\n",
    "    dataset = pd.DataFrame[\"model_output_text\"] ;  Dataset a comparar. DEBE DE SER EL ARCHIVO ORIGINAL\n",
    "    ruta = str ; Ruta para guardar\n",
    "\n",
    "    Shoutout a Diego, por su ayuda debuggeando, no solo se cagó de la risa y me humilló :c\n",
    "    \"\"\"\n",
    "    dic_aux = {0:0, 1:0.333333, 2:0.6666666, 3:1}\n",
    "    random_split = [random_interval(_) for _ in dataset[\"model_output_text\"]]\n",
    "    proba_list = random_probalist(dic_aux, random_split)\n",
    "    json_creation(dataset, proba_list, ruta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3cfb82-7938-48f6-8d61-d954fef6ffd8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CELDA 4: EVALUACIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6e4d2b4-9fa0-4429-9504-39849ff819f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recompute_hard_labels(soft_labels):\n",
    "    \"\"\"optionally, infer hard labels from the soft labels provided\"\"\"\n",
    "    hard_labels = [] \n",
    "    prev_end = -1\n",
    "    for start, end in (\n",
    "        (lbl['start'], lbl['end']) \n",
    "        for lbl in sorted(soft_labels, key=lambda span: (span['start'], span['end']))\n",
    "        if lbl['prob'] > 0.5\n",
    "    ):\n",
    "        if start == prev_end:\n",
    "            hard_labels[-1][-1] = end\n",
    "        else:\n",
    "            hard_labels.append([start, end])\n",
    "        prev_end = end\n",
    "    return hard_labels\n",
    "\n",
    "\n",
    "def infer_soft_labels(hard_labels):\n",
    "    \"\"\"reformat hard labels into soft labels with prob 1\"\"\"\n",
    "    return [\n",
    "        {\n",
    "            'start': start,\n",
    "            'end': end,\n",
    "            'prob': 1.0,\n",
    "        }\n",
    "        for start, end in hard_labels\n",
    "    ]\n",
    "\n",
    "\n",
    "def load_jsonl_file_to_records(filename, is_ref=True):\n",
    "    \"\"\"read data from a JSONL file and format that as a `pandas.DataFrame`.\n",
    "    Performs minor format checks (ensures that some labels are present,\n",
    "    optionally compute missing labels on the fly).\"\"\"\n",
    "    df = pd.read_json(filename, lines=True)\n",
    "    if not is_ref:\n",
    "        assert ('hard_labels' in df.columns) or ('soft_labels' in df.columns), \\\n",
    "            f'File {filename} contains no predicted label!'\n",
    "        if 'hard_labels' not in df.columns:\n",
    "            df['hard_labels'] = df.soft_labels.apply(recompute_hard_labels)\n",
    "        elif 'soft_labels' not in df.columns:\n",
    "            df['soft_labels'] = df.hard_labels.apply(infer_soft_labels)\n",
    "    # adding an extra column for convenience\n",
    "    columns = ['id', 'soft_labels', 'hard_labels']\n",
    "    if is_ref:\n",
    "        df['text_len'] = df.model_output_text.apply(len)\n",
    "        columns += ['text_len']\n",
    "    df = df[columns]\n",
    "    return df.sort_values('id').to_dict(orient='records')\n",
    "\n",
    "def score_iou(ref_dict, pred_dict):\n",
    "    \"\"\"computes intersection-over-union between reference and predicted hard labels, for a single datapoint.\n",
    "    inputs:\n",
    "    - ref_dict: a gold reference datapoint,\n",
    "    - pred_dict: a model's prediction\n",
    "    returns:\n",
    "    the IoU, or 1.0 if neither the reference nor the prediction contain hallucinations\n",
    "    \"\"\"\n",
    "    # ensure the prediction is correctly matched to its reference\n",
    "    assert ref_dict['id'] == pred_dict['id']\n",
    "    # convert annotations to sets of indices\n",
    "    ref_indices = {idx for span in ref_dict['hard_labels'] for idx in range(*span)}\n",
    "    pred_indices = {idx for span in pred_dict['hard_labels'] for idx in range(*span)}\n",
    "    # avoid division by zero\n",
    "    if not pred_indices and not ref_indices: return 1.\n",
    "    # otherwise compute & return IoU\n",
    "    return len(ref_indices & pred_indices) / len(ref_indices | pred_indices)\n",
    "\n",
    "def score_cor(ref_dict, pred_dict):\n",
    "    \"\"\"computes Spearman correlation between predicted and reference soft labels, for a single datapoint.\n",
    "    inputs:\n",
    "    - ref_dict: a gold reference datapoint,\n",
    "    - pred_dict: a model's prediction\n",
    "    returns:\n",
    "    the Spearman correlation, or a binarized exact match (0.0 or 1.0) if the reference or prediction contains no variation\n",
    "    \"\"\"\n",
    "    # ensure the prediction is correctly matched to its reference\n",
    "    assert ref_dict['id'] == pred_dict['id']\n",
    "    # convert annotations to vectors of observations\n",
    "    ref_vec = [0.] * ref_dict['text_len']\n",
    "    pred_vec = [0.] * ref_dict['text_len']\n",
    "    for span in ref_dict['soft_labels']:\n",
    "        for idx in range(span['start'], span['end']):\n",
    "            ref_vec[idx] = span['prob']\n",
    "    for span in pred_dict['soft_labels']:\n",
    "        for idx in range(span['start'], span['end']):\n",
    "            pred_vec[idx] = span['prob']\n",
    "    # constant series (i.e., no hallucination) => cor is undef\n",
    "    if len({round(flt, 8) for flt in pred_vec}) == 1 or len({round(flt, 8) for flt in ref_vec}) == 1 : \n",
    "        return float(len({round(flt, 8) for flt in ref_vec}) == len({round(flt, 8) for flt in pred_vec}))\n",
    "    # otherwise compute Spearman's rho\n",
    "    return spearmanr(ref_vec, pred_vec).correlation\n",
    "\n",
    "def main(ref_dicts, pred_dicts, output_file=None):\n",
    "    assert len(ref_dicts) == len(pred_dicts)\n",
    "    ious = np.array([score_iou(r, d) for r, d in zip(ref_dicts, pred_dicts)])\n",
    "    cors = np.array([score_cor(r, d) for r, d in zip(ref_dicts, pred_dicts)])\n",
    "    if output_file is not None:\n",
    "        with open(output_file, 'w') as ostr:\n",
    "            print(f'IoU: {ious.mean():.8f}', file=ostr)\n",
    "            print(f'Cor: {cors.mean():.8f}', file=ostr)\n",
    "    return ious, cors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48529795-924b-41fe-b1b3-e24276fb9269",
   "metadata": {},
   "source": [
    "## CASO DE USO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87f41b1c-8575-4206-aa26-11efcd110b81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traits  Trait\n",
      "techniques  Technique\n",
      "Stahlberg  Stahlberg (disambiguation)\n",
      "region France Vaux en Amiénois  Vaux\n",
      "Pasteur crater  Pasteur (disambiguation)\n",
      "Black Sabbath Eternal Idol  Eternal Idol\n",
      "19521 Chaos  Chaos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding guardados en la ruta C:\\Users\\FLopezP\\Documents\\GitHub\\Mu-SHROOM-GIL\\Datasets\\full_pipeline_datasets\\en_context.csv . Saludos\n",
      "CPU times: total: 13.3 s\n",
      "Wall time: 4min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Celda 1\n",
    "test_set_en = pd.read_json(r'C:\\Users\\FLopezP\\Documents\\GitHub\\Mu-SHROOM-GIL\\Datasets\\test_ds\\v1\\mushroom.en-tst.v1.jsonl', lines = True)\n",
    "full_context_pipeline(test_set_en[\"model_input\"], 'en', 2, r'C:\\Users\\FLopezP\\Documents\\GitHub\\Mu-SHROOM-GIL\\Datasets\\full_pipeline_datasets\\en_context.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "660c569f-ac6d-490f-8d35-272728135167",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se guardó el dataset de respuestas de Chapi en: C:\\Users\\FLopezP\\Documents\\GitHub\\Mu-SHROOM-GIL\\Datasets\\full_pipeline_datasets\\en_llm.csv. Viva Leo Messi gigante dios\n",
      "CPU times: total: 24min 3s\n",
      "Wall time: 17min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Celda 2\n",
    "\n",
    "en_embs = pd.read_csv(r'C:\\Users\\FLopezP\\Documents\\GitHub\\Mu-SHROOM-GIL\\Datasets\\full_pipeline_datasets\\en_context.csv')\n",
    "en_embs = en_embs.drop(columns = (\"Unnamed: 0\"))\n",
    "llm_full(en_embs, r'C:\\Users\\FLopezP\\Documents\\GitHub\\Mu-SHROOM-GIL\\Datasets\\full_pipeline_datasets\\en_llm.csv', 'llama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d8082a5e-a14c-4182-8687-c504e0038bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'lang', 'model_input', 'model_output_text', 'model_id',\n",
      "       'model_output_tokens', 'model_output_logits'],\n",
      "      dtype='object')\n",
      "---\n",
      "Index(['id', 'lang', 'model_input', 'model_output_text', 'model_id',\n",
      "       'model_output_tokens', 'model_output_logits', 'soft_labels'],\n",
      "      dtype='object')\n",
      "---\n",
      "Index(['id', 'lang', 'model_input', 'model_output_text', 'model_id',\n",
      "       'model_output_tokens', 'soft_labels', 'hard_labels',\n",
      "       'model_output_logits'],\n",
      "      dtype='object')\n",
      "---\n",
      "Se guardó el archivo en C:\\Users\\FLopezP\\Documents\\GitHub\\Mu-SHROOM-GIL\\Datasets\\full_pipeline_datasets\\en_test_dataset.jsonl. Viva Messi.\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 10 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Celda 3\n",
    "test_set_en = pd.read_json(r'C:\\Users\\FLopezP\\Documents\\GitHub\\Mu-SHROOM-GIL\\Datasets\\test_ds\\v1\\mushroom.en-tst.v1.jsonl', lines = True)\n",
    "pipeline3(test_set_en, r'C:\\Users\\FLopezP\\Documents\\GitHub\\Mu-SHROOM-GIL\\Datasets\\full_pipeline_datasets\\en_test_dataset.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e27440-3abf-4c79-9e5f-10093d06dcdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
