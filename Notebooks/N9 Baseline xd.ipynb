{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3013f025-6576-4a79-b2e9-91bccd6a970f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import argparse\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "LABEL_LIST=[0,1]\n",
    "LANGS = ['ar', 'de', 'en', 'es', 'fi', 'fr', 'hi', 'it', 'sv', 'zh']\n",
    "#LANGS = ['en']\n",
    "MODEL_NAME = 'FacebookAI/xlm-roberta-base'\n",
    "\n",
    "def tokenize_and_map_labels(examples, tokenizer):\n",
    "    tokenized_inputs = tokenizer(examples['model_output_text'], return_offsets_mapping=True, padding=True, truncation=True)\n",
    "    offset_mappings = tokenized_inputs['offset_mapping']\n",
    "    all_labels = examples['hard_labels']\n",
    "    tok_labels_batch = []\n",
    "    for batch_idx in range(len(offset_mappings)):\n",
    "        offset_mapping = offset_mappings[batch_idx]\n",
    "        hard_labels = all_labels[batch_idx]\n",
    "        tok_labels = [0] * len(offset_mapping)\n",
    "        for idx, start_end in enumerate(offset_mapping):\n",
    "            start = start_end[0]\n",
    "            end = start_end[1]\n",
    "            for (label_start, label_end) in hard_labels:\n",
    "                if start >= label_start and end <= label_end:\n",
    "                    tok_labels[idx] = 1\n",
    "        tok_labels_batch.append(tok_labels)\n",
    "    tokenized_inputs['labels'] = tok_labels_batch\n",
    "    return tokenized_inputs\n",
    "\n",
    "def train_model(test_lang, data_path, output_dir):\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, num_labels=2)  # Adjust num_labels as needed\n",
    "\n",
    "    data_files = {\n",
    "        'train': [f'{data_path}\\mushroom.{lang}-val.v2.jsonl' for lang in LANGS if lang != test_lang],\n",
    "        'validation': f'{data_path}\\mushroom.{test_lang}-val.v2.jsonl'\n",
    "    }\n",
    "    print(data_files)\n",
    "    dataset = load_dataset('json', data_files=data_files)\n",
    "    # Tokenize the dataset\n",
    "    tokenized_datasets = dataset.map(lambda x: tokenize_and_map_labels(x, tokenizer), batched=True)\n",
    "    print(\"tokenized_datasets:\\n\",tokenized_datasets)\n",
    "\n",
    "    # Prepare the dataset for training\n",
    "    train_dataset = tokenized_datasets['train']\n",
    "    eval_dataset = tokenized_datasets['validation']\n",
    "\n",
    "    # Define the training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy='epoch',\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "\n",
    "    # Define the metric\n",
    "    metric = load_metric('seqeval', trust_remote_code=True)\n",
    "\n",
    "    def compute_metrics(p):\n",
    "        predictions, labels = p\n",
    "        predictions = torch.argmax(torch.tensor(predictions), dim=2)\n",
    "        true_labels = [[LABEL_LIST[l] for l in label if l != -100] for label in labels]\n",
    "        true_predictions = [\n",
    "            [LABEL_LIST[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "\n",
    "        results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "        return {\n",
    "            \"precision\": results[\"overall_precision\"],\n",
    "            \"recall\": results[\"overall_recall\"],\n",
    "            \"f1\": results[\"overall_f1\"],\n",
    "            \"accuracy\": results[\"overall_accuracy\"],\n",
    "        }\n",
    "\n",
    "    # Initialize the Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model\n",
    "    trainer.evaluate()\n",
    "    print(f\"Model trained and evaluated successfully. Model checkpoint saved in {output_dir}\")\n",
    "\n",
    "\n",
    "def test_model(test_lang, model_path, data_path):\n",
    "    # Load the tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "    # Load the test dataset\n",
    "    test_dataset = load_dataset('json', data_files={'test': f'{data_path}\\mushroom.{test_lang}-val.v2.jsonl'})['test']\n",
    "    # Tokenize test dataset\n",
    "    inputs = tokenizer(test_dataset['model_output_text'], padding=True, truncation=True, return_offsets_mapping=True, return_tensors=\"pt\")\n",
    "\n",
    "    # Get predictions for the test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs.input_ids)\n",
    "    preds = torch.argmax(outputs.logits, dim=2)\n",
    "    probs = F.softmax(outputs.logits, dim=2)\n",
    "    # map predictions to character spans\n",
    "    hard_labels_all = {}\n",
    "    soft_labels_all = {}\n",
    "    predictions_all = []\n",
    "    for i, pred in enumerate(preds):\n",
    "        hard_labels_sample = []\n",
    "        soft_labels_sample = []\n",
    "        positive_indices = torch.nonzero(pred == 1, as_tuple=False)\n",
    "        offset_mapping = inputs['offset_mapping'][i]\n",
    "        for j, offset in enumerate(offset_mapping):\n",
    "            soft_labels_sample.append({'start': offset[0].item(), 'end': offset[1].item(), 'prob': probs[i][j][1].item()})\n",
    "            if j in positive_indices:\n",
    "                hard_labels_sample.append((offset[0].item(), offset[1].item()))\n",
    "        soft_labels_all[test_dataset['id'][i]] = soft_labels_sample\n",
    "        hard_labels_all[test_dataset['id'][i]] = hard_labels_sample\n",
    "        predictions_all.append({'id': test_dataset['id'][i], 'hard_labels': hard_labels_sample, 'soft_labels': soft_labels_sample})\n",
    "    with open(f\"{test_lang}-hard_labels.json\", 'w') as f:\n",
    "        json.dump(hard_labels_all, f)\n",
    "    with open(f\"{test_lang}-soft_labels.json\", 'w') as f:\n",
    "        json.dump(soft_labels_all, f)\n",
    "    with open(f\"{test_lang}-pred.jsonl\", 'w') as f:\n",
    "        for pred_dict in predictions_all:\n",
    "            print(json.dumps(pred_dict), file=f)\n",
    "    print(f\"Labels saved to {test_lang}-hard_labels.json and {test_lang}-soft_labels.json\")\n",
    "    print(f\"Prediction file saved to {test_lang}-pred.jsonl\")\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    if args.mode == 'train':\n",
    "        train_model(test_lang='en', data_path=args.data_path, output_dir=args.model_checkpoint,)\n",
    "    else:\n",
    "        print(f\"Test model: {args.model_checkpoint}\")\n",
    "        test_model(test_lang=args.test_lang, model_path=args.model_checkpoint, data_path=args.data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3218952-886f-4bff-8a93-5a1120951cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FLopezP\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at FacebookAI/xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': ['C:\\\\Users\\\\FLopezP\\\\Documents\\\\GitHub\\\\Mu-SHROOM-GIL\\\\Datasets\\\\val_ds\\\\mushroom.ar-val.v2.jsonl', 'C:\\\\Users\\\\FLopezP\\\\Documents\\\\GitHub\\\\Mu-SHROOM-GIL\\\\Datasets\\\\val_ds\\\\mushroom.de-val.v2.jsonl', 'C:\\\\Users\\\\FLopezP\\\\Documents\\\\GitHub\\\\Mu-SHROOM-GIL\\\\Datasets\\\\val_ds\\\\mushroom.es-val.v2.jsonl', 'C:\\\\Users\\\\FLopezP\\\\Documents\\\\GitHub\\\\Mu-SHROOM-GIL\\\\Datasets\\\\val_ds\\\\mushroom.fi-val.v2.jsonl', 'C:\\\\Users\\\\FLopezP\\\\Documents\\\\GitHub\\\\Mu-SHROOM-GIL\\\\Datasets\\\\val_ds\\\\mushroom.fr-val.v2.jsonl', 'C:\\\\Users\\\\FLopezP\\\\Documents\\\\GitHub\\\\Mu-SHROOM-GIL\\\\Datasets\\\\val_ds\\\\mushroom.hi-val.v2.jsonl', 'C:\\\\Users\\\\FLopezP\\\\Documents\\\\GitHub\\\\Mu-SHROOM-GIL\\\\Datasets\\\\val_ds\\\\mushroom.it-val.v2.jsonl', 'C:\\\\Users\\\\FLopezP\\\\Documents\\\\GitHub\\\\Mu-SHROOM-GIL\\\\Datasets\\\\val_ds\\\\mushroom.sv-val.v2.jsonl', 'C:\\\\Users\\\\FLopezP\\\\Documents\\\\GitHub\\\\Mu-SHROOM-GIL\\\\Datasets\\\\val_ds\\\\mushroom.zh-val.v2.jsonl'], 'validation': 'C:\\\\Users\\\\FLopezP\\\\Documents\\\\GitHub\\\\Mu-SHROOM-GIL\\\\Datasets\\\\val_ds\\\\mushroom.en-val.v2.jsonl'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c62adb703974cd782d56b99f41bc467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized_datasets:\n",
      " DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'lang', 'model_input', 'model_output_text', 'model_id', 'soft_labels', 'hard_labels', 'model_output_tokens', 'model_output_logits', 'input_ids', 'attention_mask', 'offset_mapping', 'labels'],\n",
      "        num_rows: 449\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'lang', 'model_input', 'model_output_text', 'model_id', 'soft_labels', 'hard_labels', 'model_output_tokens', 'model_output_logits', 'input_ids', 'attention_mask', 'offset_mapping', 'labels'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FLopezP\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\FLopezP\\AppData\\Local\\Temp\\ipykernel_16392\\3596782293.py:65: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('seqeval', trust_remote_code=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='145' max='145' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [145/145 40:01, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.193795</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.935143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.192270</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.925314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.179499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.920857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.168715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.926400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.165664</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.932571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FLopezP\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: 0 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\FLopezP\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: 1 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\FLopezP\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\FLopezP\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\FLopezP\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\numpy\\lib\\function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "C:\\Users\\FLopezP\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "C:\\Users\\FLopezP\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: 0 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\FLopezP\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: 1 seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "C:\\Users\\FLopezP\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\FLopezP\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\FLopezP\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\numpy\\lib\\function_base.py:520: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "C:\\Users\\FLopezP\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained and evaluated successfully. Model checkpoint saved in ./results\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "# Define the parser\n",
    "parser = argparse.ArgumentParser(description=\"Train or test the model\")\n",
    "parser.add_argument('--mode', type=str, choices=['train', 'test'], default='train')\n",
    "parser.add_argument('--data_path', default=r'C:\\Users\\FLopezP\\Documents\\GitHub\\Mu-SHROOM-GIL\\Datasets\\train_ds', type=str, help=\"Path to the training data\")\n",
    "parser.add_argument('--model_checkpoint', type=str, default=\"./results\", help=\"Path to the trained checkpoint\")\n",
    "parser.add_argument('--test_lang', type=str, default=\"en\")\n",
    "\n",
    "# Manually pass the arguments as a list\n",
    "args = parser.parse_args(['--mode', 'train', '--data_path', r'C:\\Users\\FLopezP\\Documents\\GitHub\\Mu-SHROOM-GIL\\Datasets\\val_ds', '--model_checkpoint', './results', '--test_lang', 'en'])\n",
    "\n",
    "# Call your main function\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85d5041f-300d-4b29-8f4e-83938a396adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test model: C:\\Users\\FLopezP\\Documents\\GitHub\\Mu-SHROOM-GIL\\Notebooks\\results\\checkpoint-145\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9cb107987e04a4a9a214158224e7f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels saved to en-hard_labels.json and en-soft_labels.json\n",
      "Prediction file saved to en-pred.jsonl\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Train or test the model\")\n",
    "parser.add_argument('--mode', type=str, choices=['train', 'test'], default='train')\n",
    "parser.add_argument('--data_path', default=r'C:\\Users\\FLopezP\\Documents\\GitHub\\Mu-SHROOM-GIL\\Datasets\\train_ds', type=str, help=\"Path to the training data\")\n",
    "parser.add_argument('--model_checkpoint', type=str, default=\"./results\", help=\"Path to the trained checkpoint\")\n",
    "parser.add_argument('--test_lang', type=str, default=\"en\")\n",
    "\n",
    "# Manually pass the arguments as a list\n",
    "#args = parser.parse_args(['--mode', 'test', '--data_path', r'C:\\Users\\FLopezP\\Documents\\GitHub\\Mu-SHROOM-GIL\\Datasets\\val_ds', '--model_checkpoint', './results', '--test_lang', 'en'])\n",
    "args = parser.parse_args(['--mode', 'test', '--data_path', r'C:\\Users\\FLopezP\\Documents\\GitHub\\Mu-SHROOM-GIL\\Datasets\\val_ds', '--model_checkpoint', r'C:\\Users\\FLopezP\\Documents\\GitHub\\Mu-SHROOM-GIL\\Notebooks\\results\\checkpoint-145', '--test_lang', 'en'])\n",
    "\n",
    "# Call your main function\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ba4299-64d6-4db5-af4c-81a5e3b7545b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
