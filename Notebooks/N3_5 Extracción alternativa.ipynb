{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d4717bcb-57e2-44cf-baf3-ae9b99b90c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e248bfe3b28c4632b46141cf4b6aee2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wikipedia\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "\n",
    "from FlagEmbedding import BGEM3FlagModel\n",
    "model = BGEM3FlagModel('BAAI/bge-m3', use_fp16 = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5adb5a6-6dc9-4ad5-960d-8adc0c1a5476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>model_id</th>\n",
       "      <th>model_input</th>\n",
       "      <th>model_output_text</th>\n",
       "      <th>model_output_logits</th>\n",
       "      <th>model_output_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN</td>\n",
       "      <td>togethercomputer/Pythia-Chat-Base-7B</td>\n",
       "      <td>Do all arthropods have antennae?</td>\n",
       "      <td>Yes, all insects and arachnids (including spi...</td>\n",
       "      <td>[-2.57427001, 5.1865358353, 5.4173498154, 2.32...</td>\n",
       "      <td>[ĠYes, ,, Ġall, Ġinsects, Ġand, Ġar, ach, n, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN</td>\n",
       "      <td>togethercomputer/Pythia-Chat-Base-7B</td>\n",
       "      <td>Do all arthropods have antennae?</td>\n",
       "      <td>Yes, all insects and arachnids have at least ...</td>\n",
       "      <td>[-2.57427001, 5.1865358353, 5.4173498154, 2.32...</td>\n",
       "      <td>[ĠYes, ,, Ġall, Ġinsects, Ġand, Ġar, ach, n, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EN</td>\n",
       "      <td>togethercomputer/Pythia-Chat-Base-7B</td>\n",
       "      <td>Do all arthropods have antennae?</td>\n",
       "      <td>Yes, all insects and arachnids (including spi...</td>\n",
       "      <td>[-2.57427001, 5.1865358353, 5.4173498154, 2.32...</td>\n",
       "      <td>[ĠYes, ,, Ġall, Ġinsects, Ġand, Ġar, ach, n, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EN</td>\n",
       "      <td>togethercomputer/Pythia-Chat-Base-7B</td>\n",
       "      <td>Do all arthropods have antennae?</td>\n",
       "      <td>Yes, all insects and arachnids (including spi...</td>\n",
       "      <td>[-2.57427001, 5.1865358353, 5.4173498154, 2.32...</td>\n",
       "      <td>[ĠYes, ,, Ġall, Ġinsects, Ġand, Ġar, ach, n, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EN</td>\n",
       "      <td>togethercomputer/Pythia-Chat-Base-7B</td>\n",
       "      <td>Do all arthropods have antennae?</td>\n",
       "      <td>Yes, all insects and arachnids (including spi...</td>\n",
       "      <td>[-2.57427001, 5.1865358353, 5.4173498154, 2.32...</td>\n",
       "      <td>[ĠYes, ,, Ġall, Ġinsects, Ġand, Ġar, ach, n, i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang                              model_id  \\\n",
       "0   EN  togethercomputer/Pythia-Chat-Base-7B   \n",
       "1   EN  togethercomputer/Pythia-Chat-Base-7B   \n",
       "2   EN  togethercomputer/Pythia-Chat-Base-7B   \n",
       "3   EN  togethercomputer/Pythia-Chat-Base-7B   \n",
       "4   EN  togethercomputer/Pythia-Chat-Base-7B   \n",
       "\n",
       "                        model_input  \\\n",
       "0  Do all arthropods have antennae?   \n",
       "1  Do all arthropods have antennae?   \n",
       "2  Do all arthropods have antennae?   \n",
       "3  Do all arthropods have antennae?   \n",
       "4  Do all arthropods have antennae?   \n",
       "\n",
       "                                   model_output_text  \\\n",
       "0   Yes, all insects and arachnids (including spi...   \n",
       "1   Yes, all insects and arachnids have at least ...   \n",
       "2   Yes, all insects and arachnids (including spi...   \n",
       "3   Yes, all insects and arachnids (including spi...   \n",
       "4   Yes, all insects and arachnids (including spi...   \n",
       "\n",
       "                                 model_output_logits  \\\n",
       "0  [-2.57427001, 5.1865358353, 5.4173498154, 2.32...   \n",
       "1  [-2.57427001, 5.1865358353, 5.4173498154, 2.32...   \n",
       "2  [-2.57427001, 5.1865358353, 5.4173498154, 2.32...   \n",
       "3  [-2.57427001, 5.1865358353, 5.4173498154, 2.32...   \n",
       "4  [-2.57427001, 5.1865358353, 5.4173498154, 2.32...   \n",
       "\n",
       "                                 model_output_tokens  \n",
       "0  [ĠYes, ,, Ġall, Ġinsects, Ġand, Ġar, ach, n, i...  \n",
       "1  [ĠYes, ,, Ġall, Ġinsects, Ġand, Ġar, ach, n, i...  \n",
       "2  [ĠYes, ,, Ġall, Ġinsects, Ġand, Ġar, ach, n, i...  \n",
       "3  [ĠYes, ,, Ġall, Ġinsects, Ġand, Ġar, ach, n, i...  \n",
       "4  [ĠYes, ,, Ġall, Ġinsects, Ġand, Ġar, ach, n, i...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_en = pd.read_json(r'C:\\Users\\FLopezP\\Documents\\GitHub\\Mu-SHROOM-GIL\\Datasets\\train_ds\\mushroom.en-train_nolabel.v1.jsonl', lines=True) #Cambiar para directorio local\n",
    "df_en.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe822aa8-49af-4723-a133-9a6c83b07200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estas tres partes están bastante bien.\n",
    "# El problema siento que radica en la siguiente celda donde recuperamos el resumen de cada página en vez de las secciones relevantes.\n",
    "def noun_list(a, lang):\n",
    "    \"\"\"\n",
    "    Filtra la pregunta y obtiene las PoST relevantes.\n",
    "    \n",
    "    a = list; Lista de preguntas del dataset\n",
    "    lang = 'es' or 'en'; Idioma a trabajar\n",
    "    \"\"\"\n",
    "    if lang == 'es':\n",
    "        post_spacy = spacy.load(\"es_core_news_sm\")\n",
    "    else:\n",
    "        post_spacy = spacy.load(\"en_core_web_sm\")\n",
    "    noun_list = []\n",
    "    nums = ['0','1','2','3','4','5','6','7','8','9']\n",
    "\n",
    "    for _ in a:\n",
    "        doc = post_spacy(_)\n",
    "        sub_noun = []\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\" or token.pos_ == \"NUM\":\n",
    "                sub_noun.append(token.text)\n",
    "            if token.pos_ == \"ADJ\" and token.text[0] in nums:\n",
    "                sub_noun.append(token.text)\n",
    "        noun_list.append(sub_noun)\n",
    "    return noun_list\n",
    "\n",
    "def keyword_por_preg(n_list):\n",
    "    \"\"\"\n",
    "    Junta lista de PoST previo a pasarlo por el API de Wikipedia.\n",
    "    \n",
    "    n_list = list; Obtenida de la función noun_list().\n",
    "    \"\"\"\n",
    "    keyword_list = []\n",
    "    for i in n_list:\n",
    "        keyword = ''\n",
    "        for j in i:\n",
    "            keyword = keyword + j + ' '\n",
    "        keyword_list.append(keyword)\n",
    "    return keyword_list\n",
    "\n",
    "def get_wikipage(text, lang, page_total):\n",
    "    \"\"\"\n",
    "    Regresa las n páginas de Wikipedia más relevantes al query\n",
    "\n",
    "    text = str; Texto proveniente de la función keyword_por_preg()\n",
    "    lang = 'es' or 'en'; Lenguaje necesario para wikipedia\n",
    "    page_total = int; Cantidad de páginas a regresar\n",
    "    \"\"\"\n",
    "    if lang == 'es':\n",
    "        wikipedia.set_lang('es')\n",
    "    if lang == 'en':\n",
    "        wikipedia.set_lang('en')\n",
    "    page_title = wikipedia.search(text, results = page_total)\n",
    "    return page_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c277961c-c4d8-4e8a-9f79-1b4bdecdfaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(wiki_names):\n",
    "    \"\"\"\n",
    "    Regresa la lista content con las páginas filtradas y segmentadas.\n",
    "\n",
    "    wiki_names = list ; Lista de los nombres de páginas de Wikipedia. Extraído del dataset. \n",
    "    \"\"\"\n",
    "    p_content = []\n",
    "    content = []\n",
    "    \n",
    "    for _ in wiki_names:\n",
    "        text = ''\n",
    "        try:\n",
    "            page = wikipedia.WikipediaPage(_)\n",
    "            textaux = page.content.replace('\\n', '')\n",
    "            textaux = textaux.replace('\\t', '')\n",
    "            text += '  ' + textaux\n",
    "        except wikipedia.exceptions.DisambiguationError:\n",
    "            print(\"Error\")\n",
    "        #text = page.content.replace('\\n', '')\n",
    "        #text = text.replace('\\t', '')\n",
    "        p_content.append(text)\n",
    "\n",
    "    for _ in p_content:\n",
    "        _ = _.split(\"===\")\n",
    "        texto = [i.replace('=', '') for i in _]\n",
    "        aux = []\n",
    "        for j in texto:\n",
    "            if len(j) > 60:\n",
    "                aux.append(j)\n",
    "        content.append(aux)\n",
    "\n",
    "    return content\n",
    "\n",
    "def embeddings_t1(seg_txt):\n",
    "    \"\"\"\n",
    "    Obtenemos los embeddings de cada elemento de seg_txt\n",
    "\n",
    "    seg_txt = list ; Instancia única de content_list (content_list[i] for i in N)\n",
    "    \"\"\"\n",
    "    len_list = [len(_) for _ in seg_txt]\n",
    "    max_len = max(len_list)\n",
    "    embedding_list = []\n",
    "    for i in seg_txt:\n",
    "        embs = model.encode(\n",
    "            i,\n",
    "            batch_size = 12,\n",
    "            max_length = max_len,\n",
    "        )[\"dense_vecs\"]\n",
    "\n",
    "        embeddings = [_ for _ in embs]\n",
    "        embedding_list.append(embeddings)\n",
    "    return embedding_list\n",
    "\n",
    "def get_content_embs(content_list):\n",
    "    \"\"\"\n",
    "    Genera la lista de content_embeddings a partir de la lista \"content\" correspondiente.\n",
    "\n",
    "    content_list  = list ; lista obtenida \n",
    "    \"\"\"\n",
    "    content_embs = [embeddings_t1(_) for _ in content_list]\n",
    "    return content_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26924c8d-f8ab-48be-93ee-29d22d7d4182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds_procesamiento(dataset, lang):\n",
    "    \"\"\"\n",
    "    Regresa un dataset bien formateado, acá chido para todo el procesamiento.\n",
    "    \n",
    "    dataset = pd.DataFrame ; El dataset mismo, así encuerado.\n",
    "    lang = str ; 'es' or 'en' Para determinar el idioma.\n",
    "    \"\"\"\n",
    "    a = keyword_por_preg(noun_list(dataset[\"model_input\"], lang))\n",
    "    dataset[\"output_filtrado\"] = a\n",
    "\n",
    "    a_set = list(set(a))\n",
    "    dic = {}\n",
    "    for _ in a_set:\n",
    "        aux = get_wikipage(_, lang, 3)\n",
    "        if not aux:\n",
    "            print(f\"Problema con: {_}\")\n",
    "        dic[f\"{_}\"] = aux\n",
    "\n",
    "    aux1 = []\n",
    "    for i in range(len(dataset[\"output_filtrado\"])):\n",
    "        if dataset[\"output_filtrado\"][i] in dic:\n",
    "            aux1.append(dic[dataset[\"output_filtrado\"][i]])\n",
    "    dataset[\"Wiki Asociado\"] = aux1\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25969ba1-c655-40d7-bcec-94dd8efc7672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preg_Emb\n",
    "def similarity(t1, t2):\n",
    "    return t1 @ t2\n",
    "\n",
    "def q_emb(pregunta):\n",
    "    \"\"\"\n",
    "    Genera embedding de una pregunta.\n",
    "\n",
    "    pregunta = dataset[\"model_input\"][i] ; Pregunta a vectorizar\n",
    "    \"\"\"\n",
    "    preb_emb = model.encode(\n",
    "        pregunta,\n",
    "        batch_size=12,\n",
    "        max_length = 512,\n",
    "    )['dense_vecs']\n",
    "    return preg_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05449cc0-2a98-49ef-988b-b33419725a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resumen_topn(texto, embeddings, pregunta, top_n):\n",
    "    \"\"\"\n",
    "    texto = list ; lista obtenida de get_content()\n",
    "    embeddings = list ; lista obtenida de get_content_embs()\n",
    "    pregunta = BGE Embedding ; Correspondiente a una pregunta del dataset.\n",
    "    top_n = int ; Cantidad de vectores usados para el resumen.\n",
    "    \"\"\"\n",
    "    full = []\n",
    "    resumen = ''\n",
    "    for i in range(len(texto)):\n",
    "        aux = list(zip(texto[i], embeddings[i]))\n",
    "        for _ in aux:\n",
    "            full.append(_)\n",
    "\n",
    "    simi_list = [(similarity(pregunta, _[1]), _[0]) for _ in full]\n",
    "    simi_list.sort(reverse=True)\n",
    "\n",
    "    for i in simi_list[:top_n]:\n",
    "        resumen += '---' + i[1]\n",
    "    return resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a2eb200-09ac-4dd9-b253-e61bcdc6e6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problema con: Bischofsheim constitutent community Mainz \n",
      "CPU times: total: 2.16 s\n",
      "Wall time: 20.3 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>model_id</th>\n",
       "      <th>model_input</th>\n",
       "      <th>model_output_text</th>\n",
       "      <th>model_output_logits</th>\n",
       "      <th>model_output_tokens</th>\n",
       "      <th>output_filtrado</th>\n",
       "      <th>Wiki Asociado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN</td>\n",
       "      <td>togethercomputer/Pythia-Chat-Base-7B</td>\n",
       "      <td>Do all arthropods have antennae?</td>\n",
       "      <td>Yes, all insects and arachnids (including spi...</td>\n",
       "      <td>[-2.57427001, 5.1865358353, 5.4173498154, 2.32...</td>\n",
       "      <td>[ĠYes, ,, Ġall, Ġinsects, Ġand, Ġar, ach, n, i...</td>\n",
       "      <td>arthropods</td>\n",
       "      <td>[Arthropod, 2018 in arthropod paleontology, Ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN</td>\n",
       "      <td>togethercomputer/Pythia-Chat-Base-7B</td>\n",
       "      <td>Do all arthropods have antennae?</td>\n",
       "      <td>Yes, all insects and arachnids have at least ...</td>\n",
       "      <td>[-2.57427001, 5.1865358353, 5.4173498154, 2.32...</td>\n",
       "      <td>[ĠYes, ,, Ġall, Ġinsects, Ġand, Ġar, ach, n, i...</td>\n",
       "      <td>arthropods</td>\n",
       "      <td>[Arthropod, 2018 in arthropod paleontology, Ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EN</td>\n",
       "      <td>togethercomputer/Pythia-Chat-Base-7B</td>\n",
       "      <td>Do all arthropods have antennae?</td>\n",
       "      <td>Yes, all insects and arachnids (including spi...</td>\n",
       "      <td>[-2.57427001, 5.1865358353, 5.4173498154, 2.32...</td>\n",
       "      <td>[ĠYes, ,, Ġall, Ġinsects, Ġand, Ġar, ach, n, i...</td>\n",
       "      <td>arthropods</td>\n",
       "      <td>[Arthropod, 2018 in arthropod paleontology, Ar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang                              model_id  \\\n",
       "0   EN  togethercomputer/Pythia-Chat-Base-7B   \n",
       "1   EN  togethercomputer/Pythia-Chat-Base-7B   \n",
       "2   EN  togethercomputer/Pythia-Chat-Base-7B   \n",
       "\n",
       "                        model_input  \\\n",
       "0  Do all arthropods have antennae?   \n",
       "1  Do all arthropods have antennae?   \n",
       "2  Do all arthropods have antennae?   \n",
       "\n",
       "                                   model_output_text  \\\n",
       "0   Yes, all insects and arachnids (including spi...   \n",
       "1   Yes, all insects and arachnids have at least ...   \n",
       "2   Yes, all insects and arachnids (including spi...   \n",
       "\n",
       "                                 model_output_logits  \\\n",
       "0  [-2.57427001, 5.1865358353, 5.4173498154, 2.32...   \n",
       "1  [-2.57427001, 5.1865358353, 5.4173498154, 2.32...   \n",
       "2  [-2.57427001, 5.1865358353, 5.4173498154, 2.32...   \n",
       "\n",
       "                                 model_output_tokens output_filtrado  \\\n",
       "0  [ĠYes, ,, Ġall, Ġinsects, Ġand, Ġar, ach, n, i...     arthropods    \n",
       "1  [ĠYes, ,, Ġall, Ġinsects, Ġand, Ġar, ach, n, i...     arthropods    \n",
       "2  [ĠYes, ,, Ġall, Ġinsects, Ġand, Ġar, ach, n, i...     arthropods    \n",
       "\n",
       "                                       Wiki Asociado  \n",
       "0  [Arthropod, 2018 in arthropod paleontology, Ar...  \n",
       "1  [Arthropod, 2018 in arthropod paleontology, Ar...  \n",
       "2  [Arthropod, 2018 in arthropod paleontology, Ar...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "aux_n = ds_procesamiento(df_en, 'en')\n",
    "aux_n.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb94e57e-24fe-4c1d-836a-b29bbb02ec55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Arthropod', '2018 in arthropod paleontology', 'Arthropod exoskeleton']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_wiki = aux_n[\"Wiki Asociado\"][0]\n",
    "temp_wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed63a5f4-098d-4796-ab29-3914c8e1edbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2.48 s\n",
      "Wall time: 3.83 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "content = get_content(temp_wiki)\n",
    "content_embs = get_content_embs(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11bbb110-13e9-442c-82a7-b53aee3ff4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "preg = df_en[\"model_input\"][0]\n",
    "p_emb = q_emb(preg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec218592-c60a-4035-af19-e4c87748e1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 1e+03 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res1 = resumen_topn(content, content_embs, p_emb, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6dd8fbb8-e1fc-4068-83ed-8bcec1aaf9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lang                                                                  EN\n",
       "model_id                            togethercomputer/Pythia-Chat-Base-7B\n",
       "model_input                             Do all arthropods have antennae?\n",
       "model_output_text       Yes, all insects and arachnids (including spi...\n",
       "model_output_logits    [-2.57427001, 5.1865358353, 5.4173498154, 2.32...\n",
       "model_output_tokens    [ĠYes, ,, Ġall, Ġinsects, Ġand, Ġar, ach, n, i...\n",
       "output_filtrado                                              arthropods \n",
       "Wiki Asociado          [Arthropod, 2018 in arthropod paleontology, Ar...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aux = df_en.iloc[0]\n",
    "aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a70ac253-7e3b-4c1d-b28d-3e5d7b59c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_aux(dataset):\n",
    "    \"\"\"\n",
    "    dataset = pd.DataFrame.iloc[i]\n",
    "    \"\"\"\n",
    "    contenido = get_content(dataset[\"Wiki Asociado\"])\n",
    "    contenido_embs = get_content_embs(contenido)\n",
    "    question = dataset[\"model_input\"]\n",
    "    question_emb = q_emb(question)\n",
    "    return resumen_topn(contenido, contenido_embs, question_emb, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "93adc09b-66b9-4a87-92be-67f322054d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n",
      "What’s the real name of Edu Manga?\n",
      "Error\n",
      "What was the previous name of the Gillette Stadium?\n",
      "Error\n",
      "What was the previous name of the Gillette Stadium?\n",
      "Error\n",
      "What was the previous name of the Gillette Stadium?\n",
      "Error\n",
      "What was the previous name of the Gillette Stadium?\n",
      "Error\n",
      "What was the previous name of the Gillette Stadium?\n",
      "Error\n",
      "What was the previous name of the Gillette Stadium?\n",
      "Error\n",
      "What was the previous name of the Gillette Stadium?\n",
      "Error\n",
      "What was the previous name of the Gillette Stadium?\n",
      "Error\n",
      "What was the previous name of the Gillette Stadium?\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'OutofMemoryError'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\FlagEmbedding\\inference\\embedder\\encoder_only\\m3.py:394\u001b[0m, in \u001b[0;36mM3Embedder.encode_single_device\u001b[1;34m(self, sentences, batch_size, max_length, return_dense, return_sparse, return_colbert_vecs, device, **kwargs)\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 394\u001b[0m     inputs_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    395\u001b[0m         all_inputs_sorted[: batch_size],\n\u001b[0;32m    396\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    397\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    398\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    399\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    400\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m    401\u001b[0m         inputs_batch,\n\u001b[0;32m    402\u001b[0m         return_dense\u001b[38;5;241m=\u001b[39mreturn_dense,\n\u001b[0;32m    403\u001b[0m         return_sparse\u001b[38;5;241m=\u001b[39mreturn_sparse,\n\u001b[0;32m    404\u001b[0m         return_colbert_vecs\u001b[38;5;241m=\u001b[39mreturn_colbert_vecs\n\u001b[0;32m    405\u001b[0m     )\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3474\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[1;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[0;32m   3472\u001b[0m \u001b[38;5;66;03m# If we have a list of dicts, let's convert it in a dict of lists\u001b[39;00m\n\u001b[0;32m   3473\u001b[0m \u001b[38;5;66;03m# We do this to allow using this method as a collate_fn function in PyTorch Dataloader\u001b[39;00m\n\u001b[1;32m-> 3474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoded_inputs, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mencoded_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, Mapping):\n\u001b[0;32m   3475\u001b[0m     encoded_inputs \u001b[38;5;241m=\u001b[39m {key: [example[key] \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m encoded_inputs] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m encoded_inputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys()}\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:4\u001b[0m\n",
      "Cell \u001b[1;32mIn[54], line 6\u001b[0m, in \u001b[0;36memb_aux\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mdataset = pd.DataFrame.iloc[i]\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m contenido \u001b[38;5;241m=\u001b[39m get_content(dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWiki Asociado\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m----> 6\u001b[0m contenido_embs \u001b[38;5;241m=\u001b[39m \u001b[43mget_content_embs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontenido\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m question \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_input\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      8\u001b[0m question_emb \u001b[38;5;241m=\u001b[39m q_emb(question)\n",
      "Cell \u001b[1;32mIn[42], line 60\u001b[0m, in \u001b[0;36mget_content_embs\u001b[1;34m(content_list)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_content_embs\u001b[39m(content_list):\n\u001b[0;32m     55\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    Genera la lista de content_embeddings a partir de la lista \"content\" correspondiente.\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03m    content_list  = list ; lista obtenida \u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     content_embs \u001b[38;5;241m=\u001b[39m [embeddings_t1(_) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m content_list]\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m content_embs\n",
      "Cell \u001b[1;32mIn[42], line 60\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_content_embs\u001b[39m(content_list):\n\u001b[0;32m     55\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    Genera la lista de content_embeddings a partir de la lista \"content\" correspondiente.\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03m    content_list  = list ; lista obtenida \u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     content_embs \u001b[38;5;241m=\u001b[39m [\u001b[43membeddings_t1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m content_list]\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m content_embs\n",
      "Cell \u001b[1;32mIn[42], line 44\u001b[0m, in \u001b[0;36membeddings_t1\u001b[1;34m(seg_txt)\u001b[0m\n\u001b[0;32m     42\u001b[0m embedding_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m seg_txt:\n\u001b[1;32m---> 44\u001b[0m     embs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdense_vecs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     50\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m [_ \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m embs]\n\u001b[0;32m     51\u001b[0m     embedding_list\u001b[38;5;241m.\u001b[39mappend(embeddings)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\FlagEmbedding\\inference\\embedder\\encoder_only\\m3.py:295\u001b[0m, in \u001b[0;36mM3Embedder.encode\u001b[1;34m(self, sentences, batch_size, max_length, return_dense, return_sparse, return_colbert_vecs, **kwargs)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_sparse \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: return_sparse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_sparse\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_colbert_vecs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: return_colbert_vecs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_colbert_vecs\n\u001b[1;32m--> 295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mencode(\n\u001b[0;32m    296\u001b[0m     sentences,\n\u001b[0;32m    297\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m    298\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m    299\u001b[0m     return_dense\u001b[38;5;241m=\u001b[39mreturn_dense,\n\u001b[0;32m    300\u001b[0m     return_sparse\u001b[38;5;241m=\u001b[39mreturn_sparse,\n\u001b[0;32m    301\u001b[0m     return_colbert_vecs\u001b[38;5;241m=\u001b[39mreturn_colbert_vecs,\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    303\u001b[0m )\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\FlagEmbedding\\abc\\inference\\AbsEmbedder.py:248\u001b[0m, in \u001b[0;36mAbsEmbedder.encode\u001b[1;34m(self, sentences, batch_size, max_length, convert_to_numpy, instruction, instruction_format, **kwargs)\u001b[0m\n\u001b[0;32m    244\u001b[0m         sentences \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_detailed_instruct(instruction_format, instruction, sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m\n\u001b[0;32m    245\u001b[0m                      sentences]\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sentences, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_devices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_single_device(\n\u001b[0;32m    249\u001b[0m         sentences,\n\u001b[0;32m    250\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m    251\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m    252\u001b[0m         convert_to_numpy\u001b[38;5;241m=\u001b[39mconvert_to_numpy,\n\u001b[0;32m    253\u001b[0m         device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_devices[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_multi_process_pool(AbsEmbedder\u001b[38;5;241m.\u001b[39m_encode_multi_process_worker)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\FlagEmbedding\\inference\\embedder\\encoder_only\\m3.py:409\u001b[0m, in \u001b[0;36mM3Embedder.encode_single_device\u001b[1;34m(self, sentences, batch_size, max_length, return_dense, return_sparse, return_colbert_vecs, device, **kwargs)\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    408\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m--> 409\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOutofMemoryError\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    410\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;66;03m# encode\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\torch\\__init__.py:2562\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m   2559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _lazy_modules:\n\u001b[0;32m   2560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m-> 2562\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch' has no attribute 'OutofMemoryError'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "resumen = []\n",
    "for i in range(len(df_en)):\n",
    "    try:\n",
    "        resumen.append(emb_aux(df_en.iloc[i]))\n",
    "    except ValueError:\n",
    "        print(df_en.iloc[i][\"model_input\"])\n",
    "        ola = \"There's is no extra information. Answer with your own information.\"\n",
    "        resumen.append(ola)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fd149c65-db3c-4a5b-ab29-977f648804a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_based_summary(dataset):\n",
    "    \"\"\"\n",
    "    dataset = pd.DataFrame ; \n",
    "    \"\"\"\n",
    "    resumen_list = []\n",
    "    for i in range(len(dataset[\"Wiki Asociado\"])):\n",
    "        contenido = get_content(dataset[\"Wiki Asociado\"][i])\n",
    "        contenido_embs = get_content_embs(contenido)\n",
    "        question = dataset[\"model_input\"][i]\n",
    "        question_emb = q_emb(question)\n",
    "        resumen = resumen_topn(contenido, contenido_embs, question_emb, 5)\n",
    "        resumen_list.append(resumen)\n",
    "    return resumen_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f7db6d23-f396-4d37-b43e-1e20d116d4d9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:1\u001b[0m\n",
      "Cell \u001b[1;32mIn[36], line 8\u001b[0m, in \u001b[0;36memb_based_summary\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWiki Asociado\u001b[39m\u001b[38;5;124m\"\u001b[39m])):\n\u001b[0;32m      7\u001b[0m     contenido \u001b[38;5;241m=\u001b[39m get_content(dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWiki Asociado\u001b[39m\u001b[38;5;124m\"\u001b[39m][i])\n\u001b[1;32m----> 8\u001b[0m     contenido_embs \u001b[38;5;241m=\u001b[39m \u001b[43mget_content_embs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontenido\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     question \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_input\u001b[39m\u001b[38;5;124m\"\u001b[39m][i]\n\u001b[0;32m     10\u001b[0m     question_emb \u001b[38;5;241m=\u001b[39m q_emb(question)\n",
      "Cell \u001b[1;32mIn[42], line 60\u001b[0m, in \u001b[0;36mget_content_embs\u001b[1;34m(content_list)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_content_embs\u001b[39m(content_list):\n\u001b[0;32m     55\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    Genera la lista de content_embeddings a partir de la lista \"content\" correspondiente.\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03m    content_list  = list ; lista obtenida \u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     content_embs \u001b[38;5;241m=\u001b[39m [embeddings_t1(_) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m content_list]\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m content_embs\n",
      "Cell \u001b[1;32mIn[42], line 60\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_content_embs\u001b[39m(content_list):\n\u001b[0;32m     55\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    Genera la lista de content_embeddings a partir de la lista \"content\" correspondiente.\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \n\u001b[0;32m     58\u001b[0m \u001b[38;5;124;03m    content_list  = list ; lista obtenida \u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     content_embs \u001b[38;5;241m=\u001b[39m [\u001b[43membeddings_t1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m content_list]\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m content_embs\n",
      "Cell \u001b[1;32mIn[42], line 41\u001b[0m, in \u001b[0;36membeddings_t1\u001b[1;34m(seg_txt)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03mObtenemos los embeddings de cada elemento de seg_txt\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03mseg_txt = list ; Instancia única de content_list (content_list[i] for i in N)\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     40\u001b[0m len_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(_) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m seg_txt]\n\u001b[1;32m---> 41\u001b[0m max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlen_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m embedding_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m seg_txt:\n",
      "\u001b[1;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ayuda = emb_based_summary(df_en)\n",
    "print(len(ayuda), len(df_en[\"Wiki Asociado\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fad4e84-9030-452c-a658-1e794688fd15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
